# ğŸ¦™ Llama å¿«é€Ÿåƒè€ƒå¡ç‰‡

> å¿«é€ŸæŒ‡ä»¤åƒè€ƒ - é©åˆæ‰“å°æˆ–ä¿å­˜

---

## ğŸš€ å•Ÿå‹•ç’°å¢ƒ

```powershell
.\venv\Scripts\Activate.ps1
```

---

## âœ… æª¢æŸ¥ç’°å¢ƒ

```powershell
.\venv\Scripts\python.exe examples\check_llama_env.py
```

---

## ğŸ’¬ å¿«é€Ÿå•ç­”ï¼ˆæœ€ç°¡å–®ï¼‰

```powershell
# å–®æ¬¡å•ç­”
.\venv\Scripts\python.exe scripts\run_inference_simple.py `
  --prompt "Your question here"

# äº¤äº’å¼
.\venv\Scripts\python.exe scripts\run_inference_simple.py

# æ¼”ç¤ºæ¨¡å¼
.\venv\Scripts\python.exe scripts\run_inference_simple.py demo
```

---

## ğŸ¯ ä½¿ç”¨æ–°ç¯„ä¾‹

### 1. å¿«é€Ÿé–‹å§‹ï¼ˆå–®ä¸€å•é¡Œï¼‰

```powershell
# CPU
.\venv\Scripts\python.exe examples\llama_quick_start.py

# GPU
.\venv\Scripts\python.exe examples\llama_quick_start.py GPU
```

### 2. äº¤äº’å¼èŠå¤©æ©Ÿå™¨äºº

```powershell
# CPU
.\venv\Scripts\python.exe examples\llama_chatbot.py

# GPU
.\venv\Scripts\python.exe examples\llama_chatbot.py GPU
```

### 3. æ‰¹é‡æ¸¬è©¦ï¼ˆå¤šå€‹å•é¡Œï¼‰

```powershell
# é è¨­å•é¡Œé›†
.\venv\Scripts\python.exe examples\llama_batch_inference.py

# GPU æ¨¡å¼
.\venv\Scripts\python.exe examples\llama_batch_inference.py GPU

# è‡ªè¨‚å•é¡Œ
.\venv\Scripts\python.exe examples\llama_batch_inference.py --custom
```

---

## ğŸ“Š æ•ˆèƒ½æ¸¬è©¦

```powershell
# CPU åŸºæº–æ¸¬è©¦
.\venv\Scripts\python.exe scripts\run_benchmark.py `
  --model ".\models\open_llama_7b_v2-int4-ov" `
  --device CPU

# GPU åŸºæº–æ¸¬è©¦
.\venv\Scripts\python.exe scripts\run_benchmark.py `
  --model ".\models\open_llama_7b_v2-int4-ov" `
  --device GPU
```

---

## ğŸ”§ å¸¸ç”¨ Python API

### åŸºæœ¬ä½¿ç”¨

```python
import openvino_genai as ov_genai

# è¼‰å…¥æ¨¡å‹
pipe = ov_genai.LLMPipeline(
    "./models/open_llama_7b_v2-int4-ov",
    "CPU"  # æˆ– "GPU"
)

# ç”Ÿæˆæ–‡æœ¬
result = pipe.generate("Your prompt", max_new_tokens=100)
print(result)
```

### é€²éšé…ç½®

```python
# å‰µæ„ç”Ÿæˆï¼ˆé«˜éš¨æ©Ÿæ€§ï¼‰
result = pipe.generate(
    prompt,
    max_new_tokens=200,
    temperature=0.9,
    top_p=0.95,
    do_sample=True
)

# ç¢ºå®šæ€§ç”Ÿæˆï¼ˆä½éš¨æ©Ÿæ€§ï¼‰
result = pipe.generate(
    prompt,
    max_new_tokens=100,
    temperature=0.1,
    do_sample=False
)
```

---

## ğŸ“¦ ä¸‹è¼‰å…¶ä»– Llama æ¨¡å‹

```powershell
# äº’å‹•å¼ï¼ˆæ¨è–¦ï¼‰
.\scripts\download_model_interactive.ps1

# å‘½ä»¤è¡Œ
python .\scripts\download_hf_model.py `
  --repo-id "meta-llama/Llama-2-7b-chat-hf"

# è½‰æ›æœ¬åœ°æ¨¡å‹
optimum-cli export openvino `
  --model meta-llama/Llama-2-7b-chat-hf `
  --weight-format int4 `
  --output-dir .\models\llama-2-7b-chat-int4 `
  --trust-remote-code
```

---

## ğŸ” æª¢æŸ¥è¨­å‚™

```powershell
# åˆ—å‡ºå¯ç”¨è¨­å‚™
python -c "import openvino as ov; print('\n'.join(ov.Core().available_devices))"

# æ‚¨çš„è¨­å‚™ï¼šCPU, GPU.0, GPU.1, NPU
```

---

## ğŸ› ç–‘é›£æ’è§£

### æ¨¡å‹æ‰¾ä¸åˆ°
```powershell
# æª¢æŸ¥æ¨¡å‹
Test-Path .\models\open_llama_7b_v2-int4-ov
ls .\models\open_llama_7b_v2-int4-ov\openvino_*.xml
```

### GPU ç„¡æ³•ä½¿ç”¨
```powershell
# æª¢æŸ¥ GPU é©…å‹•
python -c "import openvino as ov; print('GPU' in ov.Core().available_devices)"
```

### è¨˜æ†¶é«”ä¸è¶³
- ä½¿ç”¨ INT4 é‡åŒ–æ¨¡å‹
- æ¸›å°‘ `max_new_tokens`
- é—œé–‰å…¶ä»–æ‡‰ç”¨ç¨‹å¼

---

## ğŸ“š å®Œæ•´æ–‡æª”

è©³ç´°èªªæ˜è«‹åƒè€ƒï¼š
- **å®Œæ•´è¨ˆç•«ï¼š** `LLAMA_SETUP_PLAN.md`
- **å¿«é€Ÿé–‹å§‹ï¼š** `QUICKSTART.md`
- **ä¸‹è¼‰æŒ‡å—ï¼š** `DOWNLOAD_QUICK_REFERENCE.md`

---

## ğŸ“ å­¸ç¿’è·¯å¾‘

1. âœ… **æª¢æŸ¥ç’°å¢ƒ** - `check_llama_env.py`
2. ğŸš€ **å¿«é€Ÿæ¸¬è©¦** - `llama_quick_start.py`
3. ğŸ’¬ **äº¤äº’èŠå¤©** - `llama_chatbot.py`
4. ğŸ“Š **æ‰¹é‡æ¸¬è©¦** - `llama_batch_inference.py`
5. âš¡ **æ•ˆèƒ½å„ªåŒ–** - GPU æ¸¬è©¦
6. ğŸŒ **Web æ‡‰ç”¨** - Streamlit/FastAPI
7. ğŸ”§ **é€²éšåŠŸèƒ½** - RAGã€Fine-tuning

---

## âš¡ ä¸€éµæ¸¬è©¦

```powershell
# æœ€å¿«é€Ÿçš„æ¸¬è©¦æ–¹å¼
.\venv\Scripts\Activate.ps1
.\venv\Scripts\python.exe examples\llama_quick_start.py
```

---

**æœ€å¾Œæ›´æ–°ï¼š** 2026-01-09  
**ç‹€æ…‹ï¼š** âœ… ç’°å¢ƒå®Œæ•´ï¼Œå¯ç›´æ¥ä½¿ç”¨ï¼

ğŸ¦™âœ¨ ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼
