# ğŸ¦™ Llama æ¨¡å‹ + OpenVINO GenAI ä½¿ç”¨è¨ˆç•«

> **å»ºç«‹æ—¥æœŸï¼š** 2026-01-09  
> **å°ˆæ¡ˆï¼š** OpenVINO GenAI Lab  
> **ç›®æ¨™ï¼š** ä½¿ç”¨ Llama æ¨¡å‹æ­é… OpenVINO GenAI API é€²è¡Œæ–‡æœ¬ç”Ÿæˆ

---

## ğŸ“‹ ç›®éŒ„

1. [ç’°å¢ƒæª¢æŸ¥](#1-ç’°å¢ƒæª¢æŸ¥)
2. [ç’°å¢ƒè¨­ç½®è£œå……](#2-ç’°å¢ƒè¨­ç½®è£œå……)
3. [Llama æ¨¡å‹æº–å‚™](#3-llama-æ¨¡å‹æº–å‚™)
4. [OpenVINO GenAI API ä½¿ç”¨](#4-openvino-genai-api-ä½¿ç”¨)
5. [å¯¦ä½œç¯„ä¾‹](#5-å¯¦ä½œç¯„ä¾‹)
6. [é€²éšä½¿ç”¨](#6-é€²éšä½¿ç”¨)
7. [ç–‘é›£æ’è§£](#7-ç–‘é›£æ’è§£)

---

## 1. ç’°å¢ƒæª¢æŸ¥ âœ…

### 1.1 ç•¶å‰ç’°å¢ƒç‹€æ…‹

| é …ç›® | ç‹€æ…‹ | ç‰ˆæœ¬/èªªæ˜ |
|------|------|-----------|
| Python | âœ… å·²å®‰è£ | 3.11.4 |
| Virtual Environment | âœ… ä½¿ç”¨ä¸­ | venv |
| OpenVINO | âœ… å·²å®‰è£ | 2025.4.1 |
| OpenVINO GenAI | âœ… å·²å®‰è£ | 2025.4.1.0 |
| OpenVINO Tokenizers | âœ… å·²å®‰è£ | 2025.4.1.0 |
| Transformers | âœ… å·²å®‰è£ | 4.57.3 |
| Llama æ¨¡å‹ | âœ… å·²ä¸‹è¼‰ | open_llama_7b_v2-int4-ov |

### 1.2 æ¨¡å‹æª”æ¡ˆç¢ºèª

æ¨¡å‹ä½ç½®ï¼š`models/open_llama_7b_v2-int4-ov/`

å¿…è¦æª”æ¡ˆæ¸…å–®ï¼š
- âœ… `openvino_model.xml` - æ¨¡å‹çµæ§‹
- âœ… `openvino_model.bin` - æ¨¡å‹æ¬Šé‡
- âœ… `openvino_tokenizer.xml` - Tokenizer çµæ§‹
- âœ… `openvino_tokenizer.bin` - Tokenizer æ¬Šé‡
- âœ… `openvino_detokenizer.xml` - Detokenizer çµæ§‹
- âœ… `openvino_detokenizer.bin` - Detokenizer æ¬Šé‡
- âœ… `config.json` - æ¨¡å‹é…ç½®
- âœ… `tokenizer_config.json` - Tokenizer é…ç½®

**çµè«–ï¼šç’°å¢ƒå®Œæ•´ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ï¼** ğŸ‰

---

## 2. ç’°å¢ƒè¨­ç½®è£œå……

### 2.1 å•Ÿå‹•è™›æ“¬ç’°å¢ƒï¼ˆæ¯æ¬¡ä½¿ç”¨å‰ï¼‰

```powershell
# å•Ÿå‹• venv
.\venv\Scripts\Activate.ps1

# é©—è­‰ç’°å¢ƒ
python --version
python -c "import openvino_genai; print(f'OpenVINO GenAI: {openvino_genai.__version__}')"
```

### 2.2 æª¢æŸ¥å¯ç”¨è¨­å‚™

```powershell
# åˆ—å‡ºå¯ç”¨çš„æ¨ç†è¨­å‚™
.\venv\Scripts\python.exe -c "import openvino as ov; print('\n'.join(ov.Core().available_devices))"
```

### 2.3 å¯é¸ï¼šå®‰è£é¡å¤–å·¥å…·

```powershell
# å¦‚éœ€æ›´å¥½çš„çµ‚ç«¯è¼¸å‡ºï¼ˆå¯é¸ï¼‰
pip install rich colorama

# å¦‚éœ€é€²åº¦æ¢ï¼ˆå¯é¸ï¼‰
pip install tqdm
```

---

## 3. Llama æ¨¡å‹æº–å‚™

### 3.1 ç•¶å‰å¯ç”¨çš„ Llama æ¨¡å‹

**âœ… å·²å°±ç·’ï¼šOpen Llama 7B (INT4 é‡åŒ–ç‰ˆæœ¬)**
- è·¯å¾‘ï¼š`models/open_llama_7b_v2-int4-ov/`
- å¤§å°ï¼šç´„ 4GBï¼ˆINT4 é‡åŒ–ï¼‰
- ç”¨é€”ï¼šé€šç”¨æ–‡æœ¬ç”Ÿæˆ
- å„ªé»ï¼šé€Ÿåº¦å¿«ã€è¨˜æ†¶é«”ä½”ç”¨å°

### 3.2 å…¶ä»–å¯é¸çš„ Llama æ¨¡å‹ï¼ˆæœªä¾†æ“´å±•ï¼‰

| æ¨¡å‹ | HuggingFace ID | ç”¨é€” |
|------|----------------|------|
| Llama 2 7B | `meta-llama/Llama-2-7b-chat-hf` | Meta å®˜æ–¹èŠå¤©æ¨¡å‹ |
| Llama 2 13B | `meta-llama/Llama-2-13b-chat-hf` | æ›´å¼·å¤§çš„ç‰ˆæœ¬ |
| Llama 3 8B | `meta-llama/Meta-Llama-3-8B-Instruct` | æœ€æ–° Llama 3 |
| CodeLlama | `codellama/CodeLlama-7b-Instruct-hf` | ç¨‹å¼ç¢¼ç”Ÿæˆ |

### 3.3 ä¸‹è¼‰æ–°çš„ Llama æ¨¡å‹ï¼ˆå¯é¸ï¼‰

```powershell
# æ–¹æ³• 1ï¼šä½¿ç”¨äº’å‹•å¼è…³æœ¬ï¼ˆæ¨è–¦ï¼‰
.\scripts\download_model_interactive.ps1

# æ–¹æ³• 2ï¼šç›´æ¥ä¸‹è¼‰ OpenVINO æ ¼å¼
python .\scripts\download_hf_model.py --repo-id "meta-llama/Llama-2-7b-chat-hf"

# æ–¹æ³• 3ï¼šå¾ HuggingFace ä¸‹è¼‰å¾Œè½‰æ›
optimum-cli export openvino `
  --model meta-llama/Llama-2-7b-chat-hf `
  --weight-format int4 `
  --output-dir .\models\llama-2-7b-chat-int4 `
  --trust-remote-code
```

**æ³¨æ„ï¼š** Meta Llama æ¨¡å‹éœ€è¦åœ¨ HuggingFace ä¸Šæ¥å—æˆæ¬Šæ¢æ¬¾ã€‚

---

## 4. OpenVINO GenAI API ä½¿ç”¨

### 4.1 æ ¸å¿ƒ API æ¦‚è¦½

```python
import openvino_genai as ov_genai

# åŸºæœ¬ä½¿ç”¨æ¨¡å¼
pipe = ov_genai.LLMPipeline(model_path, device)
result = pipe.generate(prompt, max_new_tokens=100)
```

### 4.2 ä¸»è¦é¡åˆ¥å’Œæ–¹æ³•

#### 4.2.1 `LLMPipeline` - ä¸»è¦æ¨ç†é¡åˆ¥

```python
# åˆå§‹åŒ–
pipe = ov_genai.LLMPipeline(
    model_path: str,          # æ¨¡å‹è·¯å¾‘
    device: str = "CPU",      # è¨­å‚™ï¼šCPU, GPU, NPU
    **kwargs                  # å…¶ä»–é…ç½®
)

# ç”Ÿæˆæ–‡æœ¬
result = pipe.generate(
    prompt: str,              # è¼¸å…¥æç¤º
    max_new_tokens: int,      # æœ€å¤§ç”Ÿæˆ token æ•¸
    **generation_config       # ç”Ÿæˆé…ç½®
)
```

#### 4.2.2 ç”Ÿæˆé…ç½®åƒæ•¸

```python
generation_config = {
    "max_new_tokens": 100,       # æœ€å¤§ç”Ÿæˆé•·åº¦
    "temperature": 0.7,           # æº«åº¦ï¼ˆ0.0-1.0ï¼Œè¶Šé«˜è¶Šéš¨æ©Ÿï¼‰
    "top_p": 0.9,                 # Nucleus sampling
    "top_k": 50,                  # Top-K sampling
    "do_sample": True,            # æ˜¯å¦æ¡æ¨£
    "repetition_penalty": 1.1,    # é‡è¤‡æ‡²ç½°
}

result = pipe.generate(prompt, **generation_config)
```

### 4.3 è¨­å‚™é¸æ“‡ç­–ç•¥

| è¨­å‚™ | é©ç”¨å ´æ™¯ | å„ªé» | ç¼ºé» |
|------|----------|------|------|
| CPU | é–‹ç™¼ã€æ¸¬è©¦ | å…¼å®¹æ€§å¥½ | è¼ƒæ…¢ |
| GPU | ç”Ÿç”¢ã€æ‰¹é‡ | å¿«é€Ÿ | éœ€è¦é©…å‹• |
| NPU | é‚Šç·£è¨­å‚™ | ä½åŠŸè€— | éœ€ç¡¬é«”æ”¯æ´ |

---

## 5. å¯¦ä½œç¯„ä¾‹

### 5.1 å¿«é€Ÿé–‹å§‹ï¼šç°¡å–®å•ç­”

å»ºç«‹æª”æ¡ˆï¼š`examples/llama_quick_start.py`

```python
"""
Llama æ¨¡å‹å¿«é€Ÿé–‹å§‹
ä½¿ç”¨ OpenVINO GenAI API é€²è¡Œç°¡å–®å•ç­”
"""

import openvino_genai as ov_genai

def main():
    # è¨­å®š
    model_path = "./models/open_llama_7b_v2-int4-ov"
    device = "CPU"  # æˆ– "GPU"
    
    print(f"è¼‰å…¥æ¨¡å‹: {model_path}")
    print(f"ä½¿ç”¨è¨­å‚™: {device}\n")
    
    # åˆå§‹åŒ–ç®¡é“
    pipe = ov_genai.LLMPipeline(model_path, device)
    
    # å•é¡Œ
    prompt = "What is artificial intelligence?"
    
    print(f"å•é¡Œ: {prompt}\n")
    print("å›ç­”: ", end="", flush=True)
    
    # ç”Ÿæˆå›ç­”
    result = pipe.generate(prompt, max_new_tokens=100)
    print(result)
    
if __name__ == "__main__":
    main()
```

**åŸ·è¡Œï¼š**
```powershell
.\venv\Scripts\python.exe examples\llama_quick_start.py
```

### 5.2 é€²éšï¼šäº¤äº’å¼èŠå¤©æ©Ÿå™¨äºº

å»ºç«‹æª”æ¡ˆï¼š`examples/llama_chatbot.py`

```python
"""
Llama äº¤äº’å¼èŠå¤©æ©Ÿå™¨äºº
ä½¿ç”¨ OpenVINO GenAI API
"""

import openvino_genai as ov_genai
import sys

def chat_bot(model_path: str, device: str = "CPU"):
    """äº¤äº’å¼èŠå¤©"""
    print(f"è¼‰å…¥æ¨¡å‹: {model_path}")
    pipe = ov_genai.LLMPipeline(model_path, device)
    
    print("\n" + "="*60)
    print("ğŸ¦™ Llama èŠå¤©æ©Ÿå™¨äººï¼ˆè¼¸å…¥ 'quit' é€€å‡ºï¼‰")
    print("="*60 + "\n")
    
    while True:
        # ç²å–ç”¨æˆ¶è¼¸å…¥
        try:
            user_input = input("You: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\n\nå†è¦‹ï¼")
            break
        
        if not user_input:
            continue
        
        if user_input.lower() in ['quit', 'exit', 'bye']:
            print("\nå†è¦‹ï¼")
            break
        
        # ç”Ÿæˆå›æ‡‰
        print("Llama: ", end="", flush=True)
        try:
            response = pipe.generate(
                user_input,
                max_new_tokens=150,
                temperature=0.7,
                top_p=0.9
            )
            print(response + "\n")
        except Exception as e:
            print(f"\néŒ¯èª¤: {e}\n")

def main():
    model_path = "./models/open_llama_7b_v2-int4-ov"
    device = "CPU"
    
    if len(sys.argv) > 1:
        device = sys.argv[1]
    
    chat_bot(model_path, device)

if __name__ == "__main__":
    main()
```

**åŸ·è¡Œï¼š**
```powershell
# CPU æ¨¡å¼
.\venv\Scripts\python.exe examples\llama_chatbot.py

# GPU æ¨¡å¼
.\venv\Scripts\python.exe examples\llama_chatbot.py GPU
```

### 5.3 æ‰¹é‡è™•ç†ï¼šå¤šå•é¡Œæ¸¬è©¦

å»ºç«‹æª”æ¡ˆï¼š`examples/llama_batch_inference.py`

```python
"""
Llama æ‰¹é‡æ¨ç†
æ¸¬è©¦å¤šå€‹å•é¡Œ
"""

import openvino_genai as ov_genai
import time

def batch_inference(model_path: str, device: str = "CPU"):
    """æ‰¹é‡æ¨ç†æ¸¬è©¦"""
    print(f"è¼‰å…¥æ¨¡å‹: {model_path}")
    pipe = ov_genai.LLMPipeline(model_path, device)
    
    # æ¸¬è©¦å•é¡Œé›†
    prompts = [
        "What is machine learning?",
        "Explain the concept of neural networks.",
        "What are the benefits of artificial intelligence?",
        "How does deep learning work?",
        "What is the difference between AI and ML?"
    ]
    
    print("\n" + "="*60)
    print(f"æ‰¹é‡æ¸¬è©¦ï¼ˆå…± {len(prompts)} å€‹å•é¡Œï¼‰")
    print("="*60 + "\n")
    
    results = []
    total_time = 0
    
    for i, prompt in enumerate(prompts, 1):
        print(f"[{i}/{len(prompts)}] {prompt}")
        print("-" * 60)
        
        start_time = time.time()
        result = pipe.generate(
            prompt,
            max_new_tokens=100,
            temperature=0.7
        )
        elapsed = time.time() - start_time
        
        print(f"å›ç­”: {result}")
        print(f"â±ï¸ è€—æ™‚: {elapsed:.2f} ç§’\n")
        
        results.append({
            "prompt": prompt,
            "result": result,
            "time": elapsed
        })
        total_time += elapsed
    
    # çµ±è¨ˆ
    print("="*60)
    print(f"âœ… å®Œæˆï¼ç¸½è€—æ™‚: {total_time:.2f} ç§’")
    print(f"ğŸ“Š å¹³å‡æ¯é¡Œ: {total_time/len(prompts):.2f} ç§’")
    print("="*60)
    
    return results

def main():
    model_path = "./models/open_llama_7b_v2-int4-ov"
    device = "CPU"
    
    batch_inference(model_path, device)

if __name__ == "__main__":
    main()
```

**åŸ·è¡Œï¼š**
```powershell
.\venv\Scripts\python.exe examples\llama_batch_inference.py
```

### 5.4 ä½¿ç”¨ç¾æœ‰è…³æœ¬ï¼ˆæœ€ç°¡å–®ï¼‰

æ‚¨å·²ç¶“æœ‰ç¾æˆçš„è…³æœ¬å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼š

```powershell
# æ–¹æ³• 1ï¼šç°¡å–®æ¨ç†ï¼ˆå–®æ¬¡ï¼‰
.\venv\Scripts\python.exe scripts\run_inference_simple.py `
  --prompt "What is OpenVINO?" `
  --model ".\models\open_llama_7b_v2-int4-ov" `
  --device CPU

# æ–¹æ³• 2ï¼šäº¤äº’å¼æ¨¡å¼
.\venv\Scripts\python.exe scripts\run_inference_simple.py

# æ–¹æ³• 3ï¼šæ¼”ç¤ºæ¨¡å¼
.\venv\Scripts\python.exe scripts\run_inference_simple.py demo
```

---

## 6. é€²éšä½¿ç”¨

### 6.1 æ•ˆèƒ½å„ªåŒ–

#### 6.1.1 ä½¿ç”¨ GPU åŠ é€Ÿ

```python
# GPU æ¨ç†
pipe = ov_genai.LLMPipeline(model_path, "GPU")

# GPU é…ç½®ï¼ˆå¯é¸ï¼‰
pipe = ov_genai.LLMPipeline(
    model_path, 
    "GPU",
    config={"PERFORMANCE_HINT": "LATENCY"}  # æˆ– "THROUGHPUT"
)
```

#### 6.1.2 æ‰¹é‡è™•ç†å„ªåŒ–

```python
# æ‰¹é‡ç”Ÿæˆï¼ˆå¦‚æœ API æ”¯æ´ï¼‰
prompts = ["Q1", "Q2", "Q3"]
results = [pipe.generate(p, max_new_tokens=100) for p in prompts]
```

### 6.2 å®¢è£½åŒ–ç”Ÿæˆåƒæ•¸

```python
# å‰µæ„ç”Ÿæˆï¼ˆé«˜éš¨æ©Ÿæ€§ï¼‰
creative_config = {
    "max_new_tokens": 200,
    "temperature": 0.9,      # é«˜æº«åº¦
    "top_p": 0.95,
    "do_sample": True
}

# ç¢ºå®šæ€§ç”Ÿæˆï¼ˆä½éš¨æ©Ÿæ€§ï¼‰
deterministic_config = {
    "max_new_tokens": 100,
    "temperature": 0.1,      # ä½æº«åº¦
    "top_p": 0.9,
    "do_sample": False
}

# ä½¿ç”¨
result = pipe.generate(prompt, **creative_config)
```

### 6.3 èˆ‡å…¶ä»–å·¥å…·æ•´åˆ

#### 6.3.1 æ•´åˆ Streamlitï¼ˆWeb UIï¼‰

```python
# éœ€è¦å®‰è£ï¼špip install streamlit
import streamlit as st
import openvino_genai as ov_genai

@st.cache_resource
def load_model():
    return ov_genai.LLMPipeline("./models/open_llama_7b_v2-int4-ov", "CPU")

st.title("ğŸ¦™ Llama Chatbot")

pipe = load_model()
prompt = st.text_input("è¼¸å…¥å•é¡Œï¼š")

if st.button("ç”Ÿæˆ"):
    with st.spinner("ç”Ÿæˆä¸­..."):
        result = pipe.generate(prompt, max_new_tokens=150)
        st.write(result)
```

é‹è¡Œï¼š
```powershell
pip install streamlit
streamlit run app.py
```

#### 6.3.2 æ•´åˆ FastAPIï¼ˆREST APIï¼‰

```python
# éœ€è¦å®‰è£ï¼špip install fastapi uvicorn
from fastapi import FastAPI
import openvino_genai as ov_genai

app = FastAPI()
pipe = ov_genai.LLMPipeline("./models/open_llama_7b_v2-int4-ov", "CPU")

@app.post("/generate")
async def generate(prompt: str, max_tokens: int = 100):
    result = pipe.generate(prompt, max_new_tokens=max_tokens)
    return {"result": result}
```

é‹è¡Œï¼š
```powershell
pip install fastapi uvicorn
uvicorn api:app --reload
```

---

## 7. ç–‘é›£æ’è§£

### 7.1 å¸¸è¦‹å•é¡Œ

#### Q1: æ¨¡å‹è¼‰å…¥å¤±æ•—

```
FileNotFoundError: [Errno 2] No such file or directory
```

**è§£æ±ºæ–¹æ¡ˆï¼š**
```powershell
# æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
Test-Path .\models\open_llama_7b_v2-int4-ov

# æª¢æŸ¥å¿…è¦æª”æ¡ˆ
ls .\models\open_llama_7b_v2-int4-ov\openvino_*.xml
```

#### Q2: GPU ä¸å¯ç”¨

```
RuntimeError: GPU device is not available
```

**è§£æ±ºæ–¹æ¡ˆï¼š**
```powershell
# æª¢æŸ¥å¯ç”¨è¨­å‚™
python -c "import openvino as ov; print(ov.Core().available_devices)"

# æ”¹ç”¨ CPU
pipe = ov_genai.LLMPipeline(model_path, "CPU")
```

#### Q3: è¨˜æ†¶é«”ä¸è¶³

```
MemoryError or Out of Memory
```

**è§£æ±ºæ–¹æ¡ˆï¼š**
1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆINT4 é‡åŒ–ï¼‰
2. æ¸›å°‘ `max_new_tokens`
3. é—œé–‰å…¶ä»–ç¨‹å¼

#### Q4: ç”Ÿæˆé€Ÿåº¦æ…¢

**å„ªåŒ–ç­–ç•¥ï¼š**
```python
# 1. ä½¿ç”¨ GPU
pipe = ov_genai.LLMPipeline(model_path, "GPU")

# 2. æ¸›å°‘ç”Ÿæˆé•·åº¦
result = pipe.generate(prompt, max_new_tokens=50)  # æ¸›å°‘é•·åº¦

# 3. ä½¿ç”¨æ•ˆèƒ½æç¤º
pipe = ov_genai.LLMPipeline(
    model_path, 
    "GPU",
    config={"PERFORMANCE_HINT": "LATENCY"}
)
```

### 7.2 é™¤éŒ¯å·¥å…·

```powershell
# è©³ç´°æ—¥èªŒ
$env:OPENVINO_LOG_LEVEL="DEBUG"
python your_script.py

# æª¢æŸ¥ OpenVINO ç‰ˆæœ¬
python -c "import openvino as ov; print(ov.__version__)"

# æª¢æŸ¥ GenAI ç‰ˆæœ¬
python -c "import openvino_genai as ov_genai; print(ov_genai.__version__)"
```

---

## ğŸ“ å¿«é€ŸæŒ‡ä»¤å‚™å¿˜éŒ„

```powershell
# å•Ÿå‹•ç’°å¢ƒ
.\venv\Scripts\Activate.ps1

# å¿«é€Ÿæ¨ç†ï¼ˆä½¿ç”¨ç¾æœ‰è…³æœ¬ï¼‰
.\venv\Scripts\python.exe scripts\run_inference_simple.py --prompt "Your question here"

# äº¤äº’å¼èŠå¤©
.\venv\Scripts\python.exe scripts\run_inference_simple.py

# å»ºç«‹æ–°çš„ Python è…³æœ¬
New-Item -Path examples\my_llama_app.py -ItemType File

# åŸ·è¡Œè…³æœ¬
.\venv\Scripts\python.exe examples\my_llama_app.py

# æª¢æŸ¥ç’°å¢ƒ
python -c "import openvino_genai; print('âœ… Ready!')"
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥è¡Œå‹•

### ç«‹å³å¯åšï¼ˆ0-5 åˆ†é˜ï¼‰

1. âœ… **æ¸¬è©¦ç¾æœ‰æ¨¡å‹**
   ```powershell
   .\venv\Scripts\python.exe scripts\run_inference_simple.py demo
   ```

2. âœ… **å»ºç«‹ç¬¬ä¸€å€‹è…³æœ¬**
   - è¤‡è£½ä¸Šé¢çš„ `llama_quick_start.py` ç¯„ä¾‹
   - åŸ·è¡Œçœ‹çœ‹æ•ˆæœ

### çŸ­æœŸç›®æ¨™ï¼ˆä»Šå¤©ï¼‰

3. ğŸ“ **å»ºç«‹äº¤äº’å¼èŠå¤©æ©Ÿå™¨äºº**
   - ä½¿ç”¨ `llama_chatbot.py` ç¯„ä¾‹
   - å®¢è£½åŒ–å•å€™èªå’Œæç¤º

4. ğŸ§ª **æ¸¬è©¦ä¸åŒåƒæ•¸**
   - èª¿æ•´ `temperature`ï¼ˆ0.1 - 1.0ï¼‰
   - æ¸¬è©¦ `max_new_tokens` å½±éŸ¿

### ä¸­æœŸç›®æ¨™ï¼ˆæœ¬é€±ï¼‰

5. ğŸš€ **ä¸‹è¼‰æ›´å¤š Llama æ¨¡å‹**
   - Llama 2 Chat
   - CodeLlamaï¼ˆå¦‚æœéœ€è¦ç¨‹å¼ç¢¼ç”Ÿæˆï¼‰

6. ğŸŒ **å»ºç«‹ Web ä»‹é¢**
   - Streamlit æˆ– FastAPI
   - è®“å…¶ä»–äººä¹Ÿèƒ½ä½¿ç”¨

### é•·æœŸç›®æ¨™ï¼ˆé€²éšï¼‰

7. âš¡ **æ•ˆèƒ½å„ªåŒ–**
   - GPU åŠ é€Ÿæ¸¬è©¦
   - æ‰¹é‡è™•ç†å„ªåŒ–

8. ğŸ”§ **æ•´åˆåˆ°å°ˆæ¡ˆ**
   - RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰
   - Fine-tuning å¾®èª¿

---

## ğŸ“š åƒè€ƒè³‡æº

### å®˜æ–¹æ–‡æª”
- [OpenVINO GenAI æ–‡æª”](https://openvinotoolkit.github.io/openvino.genai/)
- [OpenVINO GenAI GitHub](https://github.com/openvinotoolkit/openvino.genai)
- [OpenVINO æ–‡æª”](https://docs.openvino.ai/)

### Llama æ¨¡å‹è³‡æº
- [Meta Llama](https://ai.meta.com/llama/)
- [Hugging Face Llama Models](https://huggingface.co/models?search=llama)
- [OpenVINO Llama ç¯„ä¾‹](https://github.com/openvinotoolkit/openvino.genai/tree/master/samples/python/chat_sample)

### ç¤¾ç¾¤è³‡æº
- [OpenVINO è«–å£‡](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)
- [GitHub Issues](https://github.com/openvinotoolkit/openvino.genai/issues)

---

## âœ… æª¢æŸ¥æ¸…å–®

### ç’°å¢ƒè¨­ç½®
- [x] Python 3.11.4 å·²å®‰è£
- [x] venv è™›æ“¬ç’°å¢ƒå·²å»ºç«‹
- [x] OpenVINO GenAI 2025.4.1.0 å·²å®‰è£
- [x] Transformers 4.57.3 å·²å®‰è£
- [x] Open Llama 7B INT4 æ¨¡å‹å·²ä¸‹è¼‰

### åŠŸèƒ½é©—è­‰
- [ ] åŸ·è¡Œç°¡å–®æ¨ç†æ¸¬è©¦
- [ ] æ¸¬è©¦äº¤äº’å¼èŠå¤©
- [ ] é©—è­‰ GPU æ¨ç†ï¼ˆå¦‚æœ‰ï¼‰
- [ ] æ¸¬è©¦æ‰¹é‡è™•ç†

### é€²éšåŠŸèƒ½
- [ ] å»ºç«‹è‡ªè¨‚æ‡‰ç”¨
- [ ] æ•´åˆ Web ä»‹é¢
- [ ] æ•ˆèƒ½å„ªåŒ–æ¸¬è©¦
- [ ] ä¸‹è¼‰å…¶ä»– Llama æ¨¡å‹

---

**æœ€å¾Œæ›´æ–°ï¼š** 2026-01-09  
**ç‹€æ…‹ï¼š** âœ… ç’°å¢ƒå®Œæ•´ï¼Œå¯ä»¥é–‹å§‹ä½¿ç”¨ï¼

**é¦¬ä¸Šé–‹å§‹ï¼š**
```powershell
.\venv\Scripts\Activate.ps1
.\venv\Scripts\python.exe scripts\run_inference_simple.py demo
```

ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼ğŸ¦™âœ¨
