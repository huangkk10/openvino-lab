"""
Llama æ‰¹é‡æ¨ç†æ¸¬è©¦
ä½¿ç”¨ OpenVINO GenAI API æ¸¬è©¦å¤šå€‹å•é¡Œ

åŸ·è¡Œæ–¹å¼ï¼š
    python examples/llama_batch_inference.py          # CPU æ¨¡å¼
    python examples/llama_batch_inference.py GPU      # GPU æ¨¡å¼
    python examples/llama_batch_inference.py --custom # è‡ªè¨‚å•é¡Œ
"""

import openvino_genai as ov_genai
import sys
import os
import time
from typing import List, Dict

def get_default_prompts() -> List[str]:
    """ç²å–é è¨­æ¸¬è©¦å•é¡Œé›†"""
    return [
        "What is machine learning?",
        "Explain the concept of neural networks in simple terms.",
        "What are the benefits of artificial intelligence?",
        "How does deep learning differ from traditional machine learning?",
        "What is the difference between AI and ML?"
    ]

def get_custom_prompts() -> List[str]:
    """ç²å–è‡ªè¨‚å•é¡Œé›†"""
    print("\n" + "=" * 70)
    print("ğŸ“ è¼¸å…¥è‡ªè¨‚å•é¡Œï¼ˆæ¯è¡Œä¸€å€‹å•é¡Œï¼Œè¼¸å…¥ç©ºè¡ŒçµæŸï¼‰")
    print("=" * 70 + "\n")
    
    prompts = []
    while True:
        try:
            line = input(f"å•é¡Œ {len(prompts) + 1}: ").strip()
            if not line:
                break
            prompts.append(line)
        except (EOFError, KeyboardInterrupt):
            print("\n")
            break
    
    return prompts if prompts else get_default_prompts()

def batch_inference(
    model_path: str,
    prompts: List[str],
    device: str = "CPU",
    max_tokens: int = 100
) -> List[Dict]:
    """åŸ·è¡Œæ‰¹é‡æ¨ç†
    
    Args:
        model_path: æ¨¡å‹è·¯å¾‘
        prompts: å•é¡Œåˆ—è¡¨
        device: æ¨ç†è¨­å‚™
        max_tokens: æœ€å¤§ç”Ÿæˆ token æ•¸
        
    Returns:
        çµæœåˆ—è¡¨ï¼ŒåŒ…å«å•é¡Œã€ç­”æ¡ˆå’ŒåŸ·è¡Œæ™‚é–“
    """
    print("=" * 70)
    print("ğŸ¦™ Llama æ‰¹é‡æ¨ç†æ¸¬è©¦")
    print("=" * 70)
    print(f"ğŸ“ æ¨¡å‹: {model_path}")
    print(f"ğŸ–¥ï¸  è¨­å‚™: {device}")
    print(f"ğŸ“Š å•é¡Œæ•¸é‡: {len(prompts)}")
    print(f"ğŸ”¢ æœ€å¤§ tokens: {max_tokens}")
    print("=" * 70 + "\n")
    
    try:
        # è¼‰å…¥æ¨¡å‹
        print("â³ è¼‰å…¥æ¨¡å‹ä¸­...")
        start_load = time.time()
        pipe = ov_genai.LLMPipeline(model_path, device)
        load_time = time.time() - start_load
        print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼(è€—æ™‚: {load_time:.2f} ç§’)\n")
        
        print("=" * 70)
        print("ğŸš€ é–‹å§‹æ‰¹é‡æ¨ç†")
        print("=" * 70 + "\n")
        
        results = []
        total_time = 0
        total_tokens = 0
        
        # é€å€‹è™•ç†å•é¡Œ
        for i, prompt in enumerate(prompts, 1):
            print(f"[{i}/{len(prompts)}] {prompt}")
            print("-" * 70)
            
            # åŸ·è¡Œæ¨ç†
            start_time = time.time()
            try:
                result = pipe.generate(
                    prompt,
                    max_new_tokens=max_tokens,
                    temperature=0.7,
                    top_p=0.9
                )
                elapsed = time.time() - start_time
                
                # ä¼°ç®— token æ•¸ï¼ˆç°¡å–®ä¼°è¨ˆï¼šå­—æ•¸ / 0.75ï¼‰
                estimated_tokens = len(result.split()) / 0.75
                
                print(f"å›ç­”: {result}")
                print(f"â±ï¸  è€—æ™‚: {elapsed:.2f} ç§’")
                print(f"ğŸ”¢ ç´„ {int(estimated_tokens)} tokens")
                print(f"âš¡ é€Ÿåº¦: {estimated_tokens/elapsed:.1f} tokens/ç§’\n")
                
                results.append({
                    "index": i,
                    "prompt": prompt,
                    "result": result,
                    "time": elapsed,
                    "tokens": int(estimated_tokens),
                    "tokens_per_sec": estimated_tokens / elapsed
                })
                
                total_time += elapsed
                total_tokens += estimated_tokens
                
            except Exception as e:
                print(f"âŒ éŒ¯èª¤: {e}\n")
                results.append({
                    "index": i,
                    "prompt": prompt,
                    "result": None,
                    "error": str(e)
                })
        
        # é¡¯ç¤ºçµ±è¨ˆ
        print("=" * 70)
        print("ğŸ“Š çµ±è¨ˆçµæœ")
        print("=" * 70)
        
        successful = [r for r in results if "error" not in r]
        failed = [r for r in results if "error" in r]
        
        print(f"\nâœ… æˆåŠŸ: {len(successful)}/{len(prompts)}")
        if failed:
            print(f"âŒ å¤±æ•—: {len(failed)}/{len(prompts)}")
        
        if successful:
            print(f"\nâ±ï¸  ç¸½è€—æ™‚: {total_time:.2f} ç§’")
            print(f"ğŸ“ˆ å¹³å‡æ¯é¡Œ: {total_time/len(successful):.2f} ç§’")
            print(f"ğŸ”¢ ç¸½ tokens: {int(total_tokens)}")
            print(f"âš¡ å¹³å‡é€Ÿåº¦: {total_tokens/total_time:.1f} tokens/ç§’")
            
            # æœ€å¿«å’Œæœ€æ…¢
            fastest = min(successful, key=lambda x: x["time"])
            slowest = max(successful, key=lambda x: x["time"])
            
            print(f"\nğŸš€ æœ€å¿«: {fastest['time']:.2f}ç§’ (å•é¡Œ {fastest['index']})")
            print(f"ğŸŒ æœ€æ…¢: {slowest['time']:.2f}ç§’ (å•é¡Œ {slowest['index']})")
        
        print("=" * 70 + "\n")
        
        return results
        
    except FileNotFoundError:
        print(f"\nâŒ éŒ¯èª¤ï¼šæ¨¡å‹ä¸å­˜åœ¨ {model_path}")
        print("è«‹å…ˆä¸‹è¼‰æ¨¡å‹ï¼Œåƒè€ƒ LLAMA_SETUP_PLAN.md")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {e}")
        sys.exit(1)

def main():
    """ä¸»å‡½æ•¸"""
    # é è¨­è¨­å®š
    model_path = "./models/open_llama_7b_v2-int4-ov"
    device = "CPU"
    use_custom = False
    
    # è§£æå‘½ä»¤è¡Œåƒæ•¸
    if len(sys.argv) > 1:
        arg = sys.argv[1].upper()
        if arg in ["--CUSTOM", "-C", "CUSTOM"]:
            use_custom = True
        elif arg in ["--HELP", "-H", "HELP"]:
            print("""
ğŸ¦™ Llama æ‰¹é‡æ¨ç†æ¸¬è©¦ - ä½¿ç”¨èªªæ˜

ç”¨æ³•:
    python examples/llama_batch_inference.py [è¨­å‚™|é¸é …]

åƒæ•¸:
    è¨­å‚™        æ¨ç†è¨­å‚™ (CPU, GPU, NPU)ï¼Œé è¨­ç‚º CPU
    --custom    ä½¿ç”¨è‡ªè¨‚å•é¡Œé›†

ç¯„ä¾‹:
    python examples/llama_batch_inference.py           # ä½¿ç”¨é è¨­å•é¡Œé›† (CPU)
    python examples/llama_batch_inference.py GPU       # ä½¿ç”¨ GPU
    python examples/llama_batch_inference.py --custom  # è‡ªè¨‚å•é¡Œ
            """)
            sys.exit(0)
        else:
            device = arg
    
    # æª¢æŸ¥æ¨¡å‹
    if not os.path.exists(model_path):
        print(f"âŒ éŒ¯èª¤ï¼šæ¨¡å‹ä¸å­˜åœ¨ {model_path}")
        print("è«‹å…ˆä¸‹è¼‰æ¨¡å‹ï¼Œåƒè€ƒ LLAMA_SETUP_PLAN.md")
        sys.exit(1)
    
    # ç²å–å•é¡Œé›†
    prompts = get_custom_prompts() if use_custom else get_default_prompts()
    
    if not prompts:
        print("âŒ æ²’æœ‰å•é¡Œå¯è™•ç†")
        sys.exit(1)
    
    # åŸ·è¡Œæ‰¹é‡æ¨ç†
    results = batch_inference(model_path, prompts, device)
    
    # å¯é¸ï¼šä¿å­˜çµæœ
    save_results = input("æ˜¯å¦ä¿å­˜çµæœåˆ°æª”æ¡ˆï¼Ÿ (y/N): ").strip().lower()
    if save_results == 'y':
        import json
        output_file = f"batch_results_{int(time.time())}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    main()
