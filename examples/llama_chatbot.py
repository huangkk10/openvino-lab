"""
Llama äº¤äº’å¼èŠå¤©æ©Ÿå™¨äºº
ä½¿ç”¨ OpenVINO GenAI API

åŸ·è¡Œæ–¹å¼ï¼š
    python examples/llama_chatbot.py          # CPU æ¨¡å¼
    python examples/llama_chatbot.py GPU      # GPU æ¨¡å¼
    python examples/llama_chatbot.py --help   # é¡¯ç¤ºå¹«åŠ©
"""

import openvino_genai as ov_genai
import sys
import os

def print_help():
    """é¡¯ç¤ºå¹«åŠ©è¨Šæ¯"""
    help_text = """
ğŸ¦™ Llama èŠå¤©æ©Ÿå™¨äºº - ä½¿ç”¨èªªæ˜

ç”¨æ³•:
    python examples/llama_chatbot.py [è¨­å‚™]

åƒæ•¸:
    è¨­å‚™    æ¨ç†è¨­å‚™ (CPU, GPU, NPU)ï¼Œé è¨­ç‚º CPU

ç¯„ä¾‹:
    python examples/llama_chatbot.py           # ä½¿ç”¨ CPU
    python examples/llama_chatbot.py GPU       # ä½¿ç”¨ GPU
    python examples/llama_chatbot.py --help    # é¡¯ç¤ºæ­¤å¹«åŠ©

èŠå¤©æŒ‡ä»¤:
    - ç›´æ¥è¼¸å…¥å•é¡Œé–‹å§‹å°è©±
    - è¼¸å…¥ 'quit', 'exit', 'bye' é€€å‡º
    - Ctrl+C ä¹Ÿå¯ä»¥é€€å‡º
    """
    print(help_text)

def chat_bot(model_path: str, device: str = "CPU"):
    """äº¤äº’å¼èŠå¤©æ©Ÿå™¨äºº
    
    Args:
        model_path: æ¨¡å‹è·¯å¾‘
        device: æ¨ç†è¨­å‚™ (CPU, GPU, NPU)
    """
    print("=" * 70)
    print("ğŸ¦™ Llama èŠå¤©æ©Ÿå™¨äºº - OpenVINO GenAI")
    print("=" * 70)
    print(f"ğŸ“ æ¨¡å‹: {model_path}")
    print(f"ğŸ–¥ï¸  è¨­å‚™: {device}")
    print("=" * 70)
    
    try:
        # è¼‰å…¥æ¨¡å‹
        print("\nâ³ è¼‰å…¥æ¨¡å‹ä¸­...")
        pipe = ov_genai.LLMPipeline(model_path, device)
        print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼\n")
        
        print("=" * 70)
        print("ğŸ’¬ é–‹å§‹å°è©±ï¼ˆè¼¸å…¥ 'quit' é€€å‡ºï¼‰")
        print("=" * 70 + "\n")
        
        # å°è©±å¾ªç’°
        while True:
            # ç²å–ç”¨æˆ¶è¼¸å…¥
            try:
                user_input = input("You: ").strip()
            except (EOFError, KeyboardInterrupt):
                print("\n\nğŸ‘‹ å†è¦‹ï¼")
                break
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºç©º
            if not user_input:
                continue
            
            # æª¢æŸ¥é€€å‡ºæŒ‡ä»¤
            if user_input.lower() in ['quit', 'exit', 'bye', 'q']:
                print("\nğŸ‘‹ å†è¦‹ï¼")
                break
            
            # ç”Ÿæˆå›æ‡‰
            print("\nğŸ¦™ Llama: ", end="", flush=True)
            try:
                response = pipe.generate(
                    user_input,
                    max_new_tokens=150,
                    temperature=0.7,
                    top_p=0.9
                )
                print(response + "\n")
            except Exception as e:
                print(f"\nâŒ ç”ŸæˆéŒ¯èª¤: {e}\n")
                
    except FileNotFoundError:
        print(f"\nâŒ éŒ¯èª¤ï¼šæ¨¡å‹ä¸å­˜åœ¨ {model_path}")
        print("è«‹å…ˆä¸‹è¼‰æ¨¡å‹ï¼Œåƒè€ƒ LLAMA_SETUP_PLAN.md")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {e}")
        print("\nç–‘é›£æ’è§£:")
        print("1. ç¢ºèªæ¨¡å‹è·¯å¾‘æ­£ç¢º")
        print("2. å¦‚ä½¿ç”¨ GPUï¼Œç¢ºèªé©…å‹•å·²å®‰è£")
        print("3. æª¢æŸ¥å¯ç”¨è¨­å‚™: python -c \"import openvino as ov; print(ov.Core().available_devices)\"")
        sys.exit(1)

def main():
    """ä¸»å‡½æ•¸"""
    # é è¨­è¨­å®š
    model_path = "./models/open_llama_7b_v2-int4-ov"
    device = "CPU"
    
    # è§£æå‘½ä»¤è¡Œåƒæ•¸
    if len(sys.argv) > 1:
        if sys.argv[1] in ["--help", "-h", "help"]:
            print_help()
            sys.exit(0)
        else:
            device = sys.argv[1].upper()
    
    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ éŒ¯èª¤ï¼šæ¨¡å‹ä¸å­˜åœ¨ {model_path}")
        print("\nè«‹å…ˆä¸‹è¼‰æ¨¡å‹:")
        print("  python scripts/download_hf_model.py --repo-id 'OpenVINO/open_llama_7b_v2-int4-ov'")
        print("\næˆ–åƒè€ƒ LLAMA_SETUP_PLAN.md ç²å–æ›´å¤šè³‡è¨Š")
        sys.exit(1)
    
    # å•Ÿå‹•èŠå¤©æ©Ÿå™¨äºº
    chat_bot(model_path, device)

if __name__ == "__main__":
    main()
