"""
Llama æ¨¡å‹å¿«é€Ÿé–‹å§‹
ä½¿ç”¨ OpenVINO GenAI API é€²è¡Œç°¡å–®å•ç­”

åŸ·è¡Œæ–¹å¼ï¼š
    python examples/llama_quick_start.py
    python examples/llama_quick_start.py --device GPU
"""

import openvino_genai as ov_genai
import sys
import os

def main():
    """å¿«é€Ÿé–‹å§‹ç¯„ä¾‹"""
    # è¨­å®š
    model_path = "./models/open_llama_7b_v2-int4-ov"
    device = "CPU"
    
    # æª¢æŸ¥å‘½ä»¤è¡Œåƒæ•¸
    if len(sys.argv) > 1:
        if sys.argv[1] in ["--device", "-d"]:
            device = sys.argv[2] if len(sys.argv) > 2 else "CPU"
        else:
            device = sys.argv[1]
    
    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ éŒ¯èª¤ï¼šæ¨¡å‹ä¸å­˜åœ¨ {model_path}")
        print("è«‹å…ˆä¸‹è¼‰æ¨¡å‹ï¼Œåƒè€ƒ LLAMA_SETUP_PLAN.md")
        sys.exit(1)
    
    print("=" * 70)
    print("ğŸ¦™ Llama å¿«é€Ÿé–‹å§‹ - OpenVINO GenAI")
    print("=" * 70)
    print(f"ğŸ“ æ¨¡å‹è·¯å¾‘: {model_path}")
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è¨­å‚™: {device}")
    print("=" * 70 + "\n")
    
    try:
        # è¼‰å…¥æ¨¡å‹
        print("â³ è¼‰å…¥æ¨¡å‹ä¸­...")
        pipe = ov_genai.LLMPipeline(model_path, device)
        print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼\n")
        
        # æ¸¬è©¦å•é¡Œ
        prompt = "What is artificial intelligence?"
        
        print(f"ğŸ’¬ å•é¡Œ: {prompt}\n")
        print("ğŸ¤– Llama å›ç­”:")
        print("-" * 70)
        
        # ç”Ÿæˆå›ç­”
        result = pipe.generate(prompt, max_new_tokens=100)
        print(result)
        
        print("-" * 70)
        print("\nâœ… æ¨ç†å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {e}")
        print("\nç–‘é›£æ’è§£:")
        print("1. ç¢ºèªæ¨¡å‹è·¯å¾‘æ­£ç¢º")
        print("2. ç¢ºèª OpenVINO GenAI å·²å®‰è£")
        print("3. å¦‚ä½¿ç”¨ GPUï¼Œç¢ºèªé©…å‹•å·²å®‰è£")
        sys.exit(1)

if __name__ == "__main__":
    main()
