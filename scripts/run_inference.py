#!/usr/bin/env python3
"""
TinyLlama æ¨ç†ç¤ºä¾‹ - ä½¿ç”¨ Transformers åº«
ä½¿ç”¨æ–¹å¼ï¼špython scripts/run_inference.py
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


def load_env_config():
    """å¾ .env æ–‡ä»¶åŠ è¼‰é…ç½®"""
    from dotenv import load_dotenv
    
    env_file = PROJECT_ROOT / "config" / ".env"
    load_dotenv(env_file)
    
    config = {
        'device': os.getenv('DEFAULT_DEVICE', 'CPU'),
        'model_path': os.getenv('DEFAULT_MODEL_PATH', './models/TinyLlama-1.1B-int4'),
        'max_tokens': int(os.getenv('MAX_NEW_TOKENS', '100')),
        'temperature': float(os.getenv('TEMPERATURE', '0.7')),
        'top_p': float(os.getenv('TOP_P', '0.9')),
    }
    
    return config


def run_inference(prompt: str, model_path: str, device: str = 'CPU', **kwargs):
    """
    åŸ·è¡Œæ¨ç†
    
    Args:
        prompt: è¼¸å…¥æç¤ºæ–‡æœ¬
        model_path: æ¨¡å‹è·¯å¾‘
        device: æ¨ç†è¨­å‚™ (CPU, GPU, NPU)
        **kwargs: å…¶ä»–æ¨ç†åƒæ•¸ (max_tokens, temperature, top_p)
    
    Returns:
        ç”Ÿæˆçš„æ–‡æœ¬
    """
    
    print(f"\n{'='*60}")
    print(f"{'TinyLlama æ¨ç†ç¤ºä¾‹':^60}")
    print(f"{'='*60}\n")
    
    # é©—è­‰æ¨¡å‹è·¯å¾‘
    model_dir = Path(model_path)
    if not model_dir.exists():
        print(f"âŒ éŒ¯èª¤ï¼šæ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨ - {model_path}")
        print(f"   è«‹å…ˆé‹è¡Œ Stage 7ï¸âƒ£ ä¸‹è¼‰æ¨¡å‹")
        print(f"   å‘½ä»¤ï¼š.\scripts\prepare_models.ps1")
        return None
    
    print(f"ğŸ“ æ¨¡å‹è·¯å¾‘: {model_path}")
    print(f"ğŸ’» æ¨ç†è¨­å‚™: {device}")
    print(f"ğŸ“ è¼¸å…¥æç¤º: {prompt}")
    print(f"âš™ï¸  åƒæ•¸è¨­ç½®:")
    for key, value in kwargs.items():
        print(f"   - {key}: {value}")
    print()
    
    try:
        # è¨­ç½®è¨­å‚™
        if device.upper() == 'GPU' and torch.cuda.is_available():
            torch_device = 'cuda'
        else:
            torch_device = 'cpu'
        
        # åŠ è¼‰åˆ†è©å™¨
        print("â³ æ­£åœ¨åŠ è¼‰åˆ†è©å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
        print("âœ… åˆ†è©å™¨åŠ è¼‰æˆåŠŸ")
        
        # åŠ è¼‰æ¨¡å‹
        print("â³ æ­£åœ¨åŠ è¼‰æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_dir,
            torch_dtype=torch.float32,
            device_map=torch_device,
            trust_remote_code=True
        )
        model.eval()
        print("âœ… æ¨¡å‹åŠ è¼‰æˆåŠŸ\n")
        
        # æº–å‚™æ¨ç†åƒæ•¸
        max_new_tokens = kwargs.get('max_tokens', 100)
        temperature = kwargs.get('temperature', 0.7)
        top_p = kwargs.get('top_p', 0.9)
        
        # ç·¨ç¢¼è¼¸å…¥
        print("â³ æ­£åœ¨æº–å‚™è¼¸å…¥...")
        inputs = tokenizer(prompt, return_tensors="pt").to(torch_device)
        print("âœ… è¼¸å…¥å·²æº–å‚™\n")
        
        # åŸ·è¡Œæ¨ç†
        print("â³ æ­£åœ¨ç”Ÿæˆæ–‡æœ¬...")
        with torch.no_grad():
            output_ids = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # è§£ç¢¼è¼¸å‡º
        result = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        
        print("\n" + "="*60)
        print("ğŸ“¤ ç”Ÿæˆçµæœ:")
        print("="*60)
        print(f"{result}")
        print("="*60 + "\n")
        
        return result
        
    except FileNotFoundError as e:
        print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {e}")
        print(f"   ç¢ºä¿æ¨¡å‹å·²åœ¨ {model_path} ç›®éŒ„ä¸­")
        return None
    except Exception as e:
        print(f"âŒ æ¨ç†éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return None


def interactive_mode():
    """äº¤äº’å¼æ¨ç†æ¨¡å¼"""
    
    # åŠ è¼‰é…ç½®
    config = load_env_config()
    model_path = PROJECT_ROOT / config['model_path']
    
    # é©—è­‰æ¨¡å‹
    if not model_path.exists():
        print(f"\nâŒ æ¨¡å‹æœªæ‰¾åˆ°: {model_path}")
        print(f"è«‹é‹è¡Œä»¥ä¸‹å‘½ä»¤ä¸‹è¼‰æ¨¡å‹:")
        print(f"  .\scripts\prepare_models.ps1")
        return
    
    print(f"\n{'='*60}")
    print(f"{'TinyLlama äº¤äº’å¼æ¨ç†':^60}")
    print(f"{'='*60}")
    print(f"\nè¨­å‚™: {config['device']}")
    print(f"æ¨¡å‹: {model_path.name}")
    print(f"æœ€å¤§ä»¤ç‰Œæ•¸: {config['max_tokens']}")
    print(f"\nè¼¸å…¥ 'exit' æˆ– 'quit' é€€å‡º\n")
    print("="*60 + "\n")
    
    try:
        # è¨­ç½®è¨­å‚™
        if config['device'].upper() == 'GPU' and torch.cuda.is_available():
            torch_device = 'cuda'
        else:
            torch_device = 'cpu'
        
        # åŠ è¼‰åˆ†è©å™¨
        print("â³ æ­£åœ¨åŠ è¼‰åˆ†è©å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        print("âœ… åˆ†è©å™¨å·²åŠ è¼‰")
        
        # åŠ è¼‰æ¨¡å‹
        print("â³ æ­£åœ¨åŠ è¼‰æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float32,
            device_map=torch_device,
            trust_remote_code=True
        )
        model.eval()
        print("âœ… æ¨¡å‹å·²åŠ è¼‰\n")
        
        while True:
            prompt = input(">>> è«‹è¼¸å…¥æç¤ºæ–‡æœ¬: ").strip()
            
            if prompt.lower() in ['exit', 'quit']:
                print("\nğŸ‘‹ å†è¦‹ï¼")
                break
            
            if not prompt:
                print("âš ï¸  æç¤ºæ–‡æœ¬ä¸èƒ½ç‚ºç©º\n")
                continue
            
            # åŸ·è¡Œæ¨ç†
            print("\nâ³ æ­£åœ¨ç”Ÿæˆ...\n")
            
            try:
                inputs = tokenizer(prompt, return_tensors="pt").to(torch_device)
                
                with torch.no_grad():
                    output_ids = model.generate(
                        **inputs,
                        max_new_tokens=config['max_tokens'],
                        temperature=config['temperature'],
                        top_p=config['top_p'],
                        do_sample=True,
                        pad_token_id=tokenizer.pad_token_id,
                        eos_token_id=tokenizer.eos_token_id
                    )
                
                result = tokenizer.decode(output_ids[0], skip_special_tokens=True)
                print(f"âœ… çµæœ: {result}\n")
                print("-" * 60 + "\n")
            except Exception as e:
                print(f"âŒ ç”Ÿæˆå¤±æ•—: {e}\n")
                print("-" * 60 + "\n")
                
    except KeyboardInterrupt:
        print("\n\nâ›” ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()


def batch_inference(prompts: list):
    """æ‰¹é‡æ¨ç†"""
    
    config = load_env_config()
    model_path = PROJECT_ROOT / config['model_path']
    
    if not model_path.exists():
        print(f"âŒ æ¨¡å‹æœªæ‰¾åˆ°: {model_path}")
        return
    
    print(f"\n{'='*60}")
    print(f"{'TinyLlama æ‰¹é‡æ¨ç†':^60}")
    print(f"{'='*60}\n")
    print(f"æ­£åœ¨è™•ç† {len(prompts)} å€‹æç¤º...\n")
    
    try:
        # è¨­ç½®è¨­å‚™
        if config['device'].upper() == 'GPU' and torch.cuda.is_available():
            torch_device = 'cuda'
        else:
            torch_device = 'cpu'
        
        # åŠ è¼‰åˆ†è©å™¨
        print("â³ æ­£åœ¨åŠ è¼‰åˆ†è©å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        print("âœ… åˆ†è©å™¨å·²åŠ è¼‰")
        
        # åŠ è¼‰æ¨¡å‹
        print("â³ æ­£åœ¨åŠ è¼‰æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float32,
            device_map=torch_device,
            trust_remote_code=True
        )
        model.eval()
        print("âœ… æ¨¡å‹å·²åŠ è¼‰\n")
        
        results = []
        
        for i, prompt in enumerate(prompts, 1):
            print(f"[{i}/{len(prompts)}] è™•ç†: {prompt[:50]}...")
            
            try:
                inputs = tokenizer(prompt, return_tensors="pt").to(torch_device)
                
                with torch.no_grad():
                    output_ids = model.generate(
                        **inputs,
                        max_new_tokens=config['max_tokens'],
                        temperature=config['temperature'],
                        top_p=config['top_p'],
                        do_sample=True,
                        pad_token_id=tokenizer.pad_token_id,
                        eos_token_id=tokenizer.eos_token_id
                    )
                
                result = tokenizer.decode(output_ids[0], skip_special_tokens=True)
                results.append({
                    'prompt': prompt,
                    'result': result
                })
                print(f"     âœ“ å®Œæˆ\n")
            except Exception as e:
                print(f"     âœ— å¤±æ•—: {e}\n")
                results.append({
                    'prompt': prompt,
                    'result': None,
                    'error': str(e)
                })
        
        print("="*60)
        print("æ‰¹é‡æ¨ç†å®Œæˆ")
        print("="*60)
        
        return results
        
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="OpenVINO GenAI æ¨ç†ç¤ºä¾‹"
    )
    
    parser.add_argument(
        'mode',
        nargs='?',
        default='interactive',
        choices=['interactive', 'demo', 'example'],
        help='é‹è¡Œæ¨¡å¼ (default: interactive)'
    )
    
    parser.add_argument(
        '--prompt',
        type=str,
        default=None,
        help='å–®æ¬¡æ¨ç†çš„æç¤ºæ–‡æœ¬'
    )
    
    parser.add_argument(
        '--device',
        type=str,
        default=None,
        choices=['CPU', 'GPU', 'NPU'],
        help='æ¨ç†è¨­å‚™'
    )
    
    parser.add_argument(
        '--max-tokens',
        type=int,
        default=None,
        help='æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•¸'
    )
    
    args = parser.parse_args()
    
    # åŠ è¼‰é…ç½®
    config = load_env_config()
    
    # è¦†è“‹å‘½ä»¤è¡Œåƒæ•¸
    if args.device:
        config['device'] = args.device
    if args.max_tokens:
        config['max_tokens'] = args.max_tokens
    
    model_path = PROJECT_ROOT / config['model_path']
    
    # é¸æ“‡é‹è¡Œæ¨¡å¼
    if args.mode == 'demo':
        # æ¼”ç¤ºæ¨¡å¼ï¼šé‹è¡Œé è¨­æç¤º
        demo_prompts = [
            "What is Python?",
            "Explain machine learning in simple terms.",
            "How does artificial intelligence work?"
        ]
        batch_inference(demo_prompts)
        
    elif args.prompt:
        # å–®æ¬¡æ¨ç†æ¨¡å¼
        run_inference(
            args.prompt,
            str(model_path),
            device=config['device'],
            max_tokens=config['max_tokens'],
            temperature=config['temperature'],
            top_p=config['top_p']
        )
        
    else:
        # äº¤äº’å¼æ¨¡å¼ï¼ˆé»˜èªï¼‰
        interactive_mode()
