#!/usr/bin/env python3
"""
TinyLlama ç°¡å–®æ¨ç† - ç›´æ¥å¾ HuggingFace åŠ è¼‰
ä½¿ç”¨æ–¹å¼ï¼špython scripts/run_inference_simple.py
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def load_env_config():
    """å¾ .env æ–‡ä»¶åŠ è¼‰é…ç½®"""
    from dotenv import load_dotenv
    
    env_file = PROJECT_ROOT / "config" / ".env"
    load_dotenv(env_file)
    
    config = {
        'device': os.getenv('DEFAULT_DEVICE', 'CPU'),
        'max_tokens': int(os.getenv('MAX_NEW_TOKENS', '100')),
        'temperature': float(os.getenv('TEMPERATURE', '0.7')),
        'top_p': float(os.getenv('TOP_P', '0.9')),
    }
    
    return config

def run_inference(prompt: str, **kwargs):
    """åŸ·è¡Œæ¨ç†"""
    
    print(f"\n{'='*60}")
    print(f"{'TinyLlama æ¨ç†ç¤ºä¾‹':^60}")
    print(f"{'='*60}\n")
    
    print(f"ğŸ“ è¼¸å…¥æç¤º: {prompt}")
    print(f"âš™ï¸  åƒæ•¸è¨­ç½®:")
    for key, value in kwargs.items():
        print(f"   - {key}: {value}")
    print()
    
    try:
        # è¨­ç½®è¨­å‚™
        if kwargs.get('device', 'CPU').upper() == 'GPU' and torch.cuda.is_available():
            torch_device = 'cuda'
            print(f"ğŸ’» æ¨ç†è¨­å‚™: GPU (CUDA)")
        else:
            torch_device = 'cpu'
            print(f"ğŸ’» æ¨ç†è¨­å‚™: CPU")
        print()
        
        # æ¨¡å‹ ID
        model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
        
        # åŠ è¼‰åˆ†è©å™¨
        print("â³ æ­£åœ¨åŠ è¼‰åˆ†è©å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        print("âœ… åˆ†è©å™¨åŠ è¼‰æˆåŠŸ")
        
        # åŠ è¼‰æ¨¡å‹
        print("â³ æ­£åœ¨åŠ è¼‰æ¨¡å‹ï¼ˆé¦–æ¬¡æœƒä¸‹è¼‰ï¼Œç´„ 2.2GBï¼‰...")
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            device_map=torch_device,
            trust_remote_code=True,
            dtype=torch.float16 if torch_device == 'cuda' else torch.float32
        )
        print("âœ… æ¨¡å‹åŠ è¼‰æˆåŠŸ\n")
        
        # æº–å‚™æ¨ç†åƒæ•¸
        max_new_tokens = kwargs.get('max_tokens', 100)
        temperature = kwargs.get('temperature', 0.7)
        top_p = kwargs.get('top_p', 0.9)
        
        # ä½¿ç”¨ Chat æ¨¡æ¿æ ¼å¼åŒ–è¼¸å…¥
        print("â³ æ­£åœ¨æº–å‚™è¼¸å…¥ï¼ˆä½¿ç”¨ Chat æ¨¡æ¿ï¼‰...")
        messages = [
            {"role": "system", "content": "You are a helpful AI assistant. Provide clear and informative answers."},
            {"role": "user", "content": prompt}
        ]
        
        # æ‡‰ç”¨ chat æ¨¡æ¿
        formatted_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = tokenizer(formatted_prompt, return_tensors="pt").to(torch_device)
        print("âœ… è¼¸å…¥å·²æº–å‚™\n")
        
        # åŸ·è¡Œæ¨ç†
        print("â³ æ­£åœ¨ç”Ÿæˆæ–‡æœ¬...")
        with torch.no_grad():
            output_ids = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # è§£ç¢¼è¼¸å‡º
        result = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        
        print("\n" + "="*60)
        print("ğŸ“¤ ç”Ÿæˆçµæœ:")
        print("="*60)
        print(f"{result}")
        print("="*60 + "\n")
        
        return result
        
    except Exception as e:
        print(f"âŒ æ¨ç†éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return None

def interactive_mode():
    """äº¤äº’å¼æ¨ç†"""
    
    config = load_env_config()
    
    print(f"\n{'='*60}")
    print(f"{'TinyLlama äº¤äº’å¼æ¨ç†':^60}")
    print(f"{'='*60}")
    print(f"\nè¨­å‚™: {config['device']}")
    print(f"æœ€å¤§ä»¤ç‰Œæ•¸: {config['max_tokens']}")
    print(f"æº«åº¦: {config['temperature']}")
    print(f"\nè¼¸å…¥ 'exit' æˆ– 'quit' é€€å‡º\n")
    print("="*60 + "\n")
    
    try:
        # è¨­ç½®è¨­å‚™
        if config['device'].upper() == 'GPU' and torch.cuda.is_available():
            torch_device = 'cuda'
        else:
            torch_device = 'cpu'
        
        model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
        
        # åŠ è¼‰åˆ†è©å™¨
        print("â³ æ­£åœ¨åŠ è¼‰åˆ†è©å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        print("âœ… åˆ†è©å™¨å·²åŠ è¼‰")
        
        # åŠ è¼‰æ¨¡å‹
        print("â³ æ­£åœ¨åŠ è¼‰æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            device_map=torch_device,
            trust_remote_code=True,
            dtype=torch.float16 if torch_device == 'cuda' else torch.float32
        )
        print("âœ… æ¨¡å‹å·²åŠ è¼‰\n")
        
        while True:
            prompt = input(">>> è«‹è¼¸å…¥æç¤ºæ–‡æœ¬: ").strip()
            
            if prompt.lower() in ['exit', 'quit']:
                print("\nğŸ‘‹ å†è¦‹ï¼")
                break
            
            if not prompt:
                print("âš ï¸  æç¤ºæ–‡æœ¬ä¸èƒ½ç‚ºç©º\n")
                continue
            
            # åŸ·è¡Œæ¨ç†
            print("\nâ³ æ­£åœ¨ç”Ÿæˆ...\n")
            
            try:
                # ä½¿ç”¨ Chat æ¨¡æ¿æ ¼å¼åŒ–
                messages = [
                    {"role": "system", "content": "You are a helpful AI assistant. Provide clear and informative answers."},
                    {"role": "user", "content": prompt}
                ]
                
                formatted_prompt = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                
                inputs = tokenizer(formatted_prompt, return_tensors="pt").to(torch_device)
                
                with torch.no_grad():
                    output_ids = model.generate(
                        **inputs,
                        max_new_tokens=config['max_tokens'],
                        temperature=config['temperature'],
                        top_p=config['top_p'],
                        do_sample=True,
                        pad_token_id=tokenizer.pad_token_id,
                        eos_token_id=tokenizer.eos_token_id
                    )
                
                result = tokenizer.decode(output_ids[0], skip_special_tokens=True)
                print(f"âœ… çµæœ:\n{result}\n")
                print("-" * 60 + "\n")
            except Exception as e:
                print(f"âŒ ç”Ÿæˆå¤±æ•—: {e}\n")
                print("-" * 60 + "\n")
                
    except KeyboardInterrupt:
        print("\n\nâ›” ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="TinyLlama æ¨ç†ç¤ºä¾‹")
    parser.add_argument('mode', nargs='?', default='interactive', 
                        choices=['interactive', 'demo'],
                        help='é‹è¡Œæ¨¡å¼')
    parser.add_argument('--prompt', type=str, default=None, help='å–®æ¬¡æ¨ç†çš„æç¤ºæ–‡æœ¬')
    parser.add_argument('--device', type=str, default=None, 
                        choices=['CPU', 'GPU'], help='æ¨ç†è¨­å‚™')
    parser.add_argument('--max-tokens', type=int, default=None, help='æœ€å¤§ä»¤ç‰Œæ•¸')
    
    args = parser.parse_args()
    config = load_env_config()
    
    if args.device:
        config['device'] = args.device
    if args.max_tokens:
        config['max_tokens'] = args.max_tokens
    
    if args.mode == 'demo':
        # æ¼”ç¤ºæ¨¡å¼
        demo_prompts = [
            "What is Python?",
            "Explain machine learning in simple terms.",
            "How does artificial intelligence work?"
        ]
        
        for i, prompt in enumerate(demo_prompts, 1):
            print(f"\n[{i}/3] æ¼”ç¤ºæç¤º:")
            run_inference(prompt, **config)
            
    elif args.prompt:
        # å–®æ¬¡æ¨ç†
        run_inference(args.prompt, **config)
    else:
        # äº¤äº’å¼æ¨¡å¼
        interactive_mode()
